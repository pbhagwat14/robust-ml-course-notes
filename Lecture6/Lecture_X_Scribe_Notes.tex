% -----------------------------*- LaTeX -*------------------------------
\documentclass[11pt]{report}
\usepackage{scribe_ds603}

\usepackage{xcolor}

\newcommand{\ab}[1]{\textcolor{purple}{[#1 - AB]}}
\newcommand{\abedit}[1]{\textcolor{purple}{#1}}


\begin{document}


\scribe{Ritik}		% required
\lecturenumber{6}			% required, must be a number
\lecturedate{22/08/2025}		% required, omit year

\maketitle

% ----------------------------------------------------------------------


\section{Revisiting the standard supervised learning setup}
\subsection{Training vs. Testing}
During \textbf{training/estimation}: We have data points
\[
    z_i = (x_i, y_i) \sim \mathbb{P^*}, \quad i = 1, \dots, n
\] 
where $\mathbb{P^*}$ is the true data distribution. Using this training set, we
contruct the \textbf{empirical distribution} which is defined as
\[
    \hat{\mathbb{P}}_n = \frac{1}{n} \sum_{i=1}^n \delta_{z_i}.
\]
Here, $\delta_{z_i}$, is the \textbf{Dirac measure}~\cite{lax2014functional} at
the point $z_i$ (a probability mass concentrated at that sample). \\
\\
During \textbf{testing/inference}: Samples $z=(x,y)$ are assumed to be drawn
from the true distribution $\mathbb{P^*}$.

\subsection{Dirac Measure}
For a measurable space $(X, \mathcal{F})$, where $\mathcal{F}$ is an
appropriately defined $\sigma$-algebra, the Dirac measure at a point $x \in X$
is defined as
\[
    \delta_x(A) = \mathbf{1}_A(x) = 
    \begin{cases}
        1 & \text{if } x \in A, \\
        0 & \text{if } x \notin A,
    \end{cases}
    \quad \forall A \in \mathcal{F}.
\]
\underline{\textbf{Note:}} The Dirac measure is a \emph{probability measure}.
For any measurable function $f$,
\[
    \int_{x} f(y)\, d\delta_x(y) = f(x).
\]
See Appendix~\ref{adx1} for proof.\\ \\
\noindent \textbf{HW:} Is $\hat{\mathbb{P}}_n$ a discrete measure?

Yes it is a discrete measure, because it is a finite sum of weighted Dirac
measures. A general discrete measure can be expressed as
\[
    \mu = \sum_{i} a_i \, \delta_{x_i}, \quad a_i \geq 0.
\]

The empirical distribution $\hat{\mathbb{P}}_n$ is a discrete measure with
uniform weights $a_i = \tfrac{1}{n} \text{ and also note} \sum_{i} a_i = 1$.

\subsection{Learning via Optimization}
The \textbf{true (or population) risk minimization} problem is
\[
    \min_{\theta \in \Theta} \, \mathbb{E}_{z \sim \mathbb{P^*}} \big[ \ell(\theta, z) \big] 
    = \min_{\theta} \int_z \ell(\theta, z) \, d\mathbb{P^*}(z).
\]
\ab{what is?}That is \textbf{expected loss} under the true distribution $\mathbb{P^*}$. Since
$\mathbb{P^*}$ is unknown, we approximate using the \textbf{empirical risk}:
\[
    \min_{\theta \in \Theta} \, \mathbb{E}_{z \sim \hat{\mathbb{P}}_n} \big[ \ell(\theta, z) \big] 
    = \frac{1}{n} \sum_{i=1}^n \ell(\theta, z_i).
\]

This is the principle of \textbf{Empirical Risk Minimization (ERM)}~\cite{uml}.

\subsection{Parameter Estimation}
The goal is to learn $\hat{\theta} : \mathcal{P}(z) \to \Theta$ such that \ab{What is $\mathcal{P}$}
\[
    \mathbb{E}_{z \sim \mathbb{P^*}} \big[ \ell(\hat{\theta}(\hat{\mathbb{P}}_n), z) \big]
\]
is minimized. \abedit{In general, this is the framework of
M-estimation~\cite{van2000asymptotic} (a generalization of maximum-likelihood
estimation), where different choices of $\ell$ lead to different statistics of
the underlying distribution such as the median arising from $\ell=|\theta -
z|$.}

\noindent \textbf{Example (Squared Loss):}
Using squared loss 
\[
    \ell(\theta, z) = \|\theta - z\|^2,
\]
the optimal $\theta$ corresponds to the \textbf{mean} of the distribution. 

\section{Data-Driven Decision Making Cycle}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{images/img1.png}
    \caption{Data-Driven Decision Making Cycle}
    \label{fig1}
\end{figure}
In the conventional data-driven decision-making cycle given in Figure~\ref{fig1}
\ab{add citation}, we observe $n$ i.i.d.\ samples generated from the unknown
data-generating distribution $\mathbb{P}^\star$, and build a model
$\hat{\mathbb{P}}_n$ from these samples (model could be parametric or
non-parametric). Based on $\hat{\mathbb{P}}_n$, we make a decision (e.g.,
parameter estimation), which is then deployed in an \textbf{out-of-sample
environment} $\tilde{\mathbb{P}}$ that may or may not coincide with the original
distribution $\mathbb{P}_\star$.

\subsection{Potential Sources of Sub-optimal Decision-Making}
Several factors can lead to suboptimal performance in this cycle:

\begin{enumerate}

    \item \textbf{Overfitting} ($\tilde{\mathbb{P}} \neq \hat{\mathbb{P}}_n$):  
    When the sample size $n$ is not sufficiently large, the learned model
    $\hat{\mathbb{P}}_n$ may fit the training samples well but fail to
    generalize to $\tilde{\mathbb{P}}$, resulting in poor out-of-sample
    performance.  

    \item \textbf{Distributional Shift} ($\tilde{\mathbb{P}} \neq
    \mathbb{P}^\star = \overline{\mathbb{P}}$):  
    In many real-world settings, the deployment distribution
    $\tilde{\mathbb{P}}$ deviates from the data-generating distribution
    $\mathbb{P}^\star$. This discrepancy may arise in scenarios such as:
    \begin{itemize}[left=1em]
        \item \textit{Adversarial deployment}: Malicious actors manipulate the
        data distribution to degrade model performance.
        \item \textit{Transfer learning}: Models must generalize to target
        datasets that differ from the source distribution.
    \end{itemize}

    \item \textbf{Data Contamination} ($\tilde{\mathbb{P}} = \mathbb{P}^\star
    \neq \overline{\mathbb{P}}$):  
    The observed data may contain outliers or measurement errors during the data
    generation and collection process. Thus, the effective training distribution
    is a contaminated version $\overline{\mathbb{P}}$ of the true
    $\mathbb{P}_\star$. This discrepancy may also arise due to model poisoning.

    \item \textbf{Backdoor Attacks} ($\tilde{\mathbb{P}} = \overline{\mathbb{P}}
    \neq \mathbb{P}^\star$): These occur when an adversary poisons the training
    data by inserting specific "triggers" (e.g., small patterns or
    perturbations) into a subset of examples with manipulated labels. The model
    thus learns a hidden rule: it behaves correctly on clean inputs, but
    produces attacker-chosen outputs whenever the trigger is present. In this
    setting, both training and deployment data distributions are corrupted
    with the trigger and differ from the true clean distribution. This makes
    detection difficult, as the poisoned model performs well on standard
    evaluation but fails under the triggered inputs.
    
\end{enumerate}

\subsection{Pre-decision vs. Post-decision Errors}
\begin{itemize}[left=1em]
    \item Cases (i) and (ii) (\textbf{overfitting and distributional shift})
    occur in the \textbf{post-decision stage}, when the trained model is
    deployed in the out-of-sample environment.
    \item Cases (iii) and (iv) (\textbf{data contamination and backdoor
    attacks}) occurs in the \textbf{pre-decision stage}, during data generation
    and collection.
\end{itemize}

\subsection{Data contamination methods}
Let $\mathbb{P}^*$ denote the true underlying data-generating distribution. We
are ultimately interested in learning a model $\hat{\mathbb{P}}_n$ that is close
to $\mathbb{P}^*$. However, data contamination can occur at different stages of
the pipeline as shown in Figure~\ref{fig2}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{images/img2.png}
    \caption{Decision Making Cycle in the Context of Robust Statistics}
    \label{fig2}
\end{figure}
In both cases, the learned model is constructed from corrupted information but
is still intended to perform well or follow the true distribution $\mathbb{P}^*$. 

\section{Distributionally Robust Optimization (DRO)}
To address cases (i) and (ii), \textbf{distributionally robust optimization
(DRO)}~\cite{rahimian2019distributionally} is a principled framework that seeks
to minimize discrepancies between the \textit{in-sample expected loss} (under
$\mathbb{P}^*$) and the \textit{out-of-sample expected loss} (under
$\tilde{\mathbb{P}}$). DRO explicitly accounts for distributional uncertainty to
achieve more robust decision-making.

\subsection{Stochastic Optimization Setup}
\begin{itemize}[left=1em]
    \item Random vector: $\xi \in \Xi \subseteq \mathbb{R}^d$, let $\vec{z} =
    \xi$, distributed according to the true distribution $\mathbb{P}^\star$.  
    \item Model parameters: $\theta \in \Theta$ (assume finite-dimensional).  
    \item Loss function: $\ell(\vec{z}, \vec{\theta})$ measures the cost of
    using parameter $\vec{\theta}$ on data realization $\vec{z}$. \abedit{In a
    classification setting, $\vec{z}=(\vec{x},y)$, and loss is measured by the dispre}  
\end{itemize}

\subsection{DRO Formulation}
Standard ERM may lead to poor out-of-sample performance due to overfitting,
distributional shift, or data contamination.  
To ensure robustness, DRO introduces an \textbf{uncertainty set}
$\mathcal{B}(\hat{\mathbb{P}}_n)$ around the empirical distribution, which
captures possible deviations between in-sample and out-of-sample distributions.
The DRO problem is then formulated as:
\begin{equation}
    \min_{\vec{\theta} \in \Theta} \; \sup_{Q \in \mathcal{B}(\hat{\mathbb{P}}_n)} 
    \mathbb{E}_{Q} \big[ \ell(\vec{z}, \vec{\theta}) \big] = \hat{L}_r(\vec{z}, \vec{\theta}),
\end{equation}

where the inner $\sup$ considers the worst-case expected loss under all
distributions $Q$ in the uncertainty set $\mathcal{B}(\hat{\mathbb{P}}_n)$.
% \newpage 
\section{Distributional Uncertainty Sets}
\textbf{Key Question:} How do we measure distributional uncertainty? \\
A key step in distributionally robust optimization (DRO) is the specification of
the \textbf{distributional uncertainty set} $\mathcal{B}$. Different choices of
$\mathcal{B}$ lead to different formulations of the DRO problem.

\subsection{Moment-Based Uncertainty Sets}
One of the earliest approaches~\cite{delage2010distributionally} models
uncertainty using \textbf{moment constraints}.  
The general DRO problem is
\[
    \min_{\theta \in \Theta} \; \sup_{\mathbb{Q} \in \mathcal{B}} \, \mathbb{E}_\mathbb{Q}[\ell(\theta, z)],
\]
where the uncertainty set $\mathcal{B}$ is defined via constraints on the mean
and covariance.

\paragraph{Construction of $\mathcal{B}$:} Let $\hat{\mu}$ and $\hat{\Sigma}$
denote the empirical mean and covariance estimated from $\hat{\mathbb{P}}_n$.  
Given non-negative constants $\delta_1, \delta_2$, define
\[
    \mathcal{B} = \Bigg\{ \mathbb{Q} \;:\;
    \big(\mathbb{E}_\mathbb{Q}[z] - \hat{\mu}\big)^\top \hat{\Sigma}^{-1}\big(\mathbb{E}_\mathbb{Q}[z] - \hat{\mu}\big) \leq \delta_1, \;
    \mathbb{E}_\mathbb{Q}\!\left[(z - \hat{\mu})^\top(z - \hat{\mu}) \right] \preceq \delta_2 \hat{\Sigma}
    \Bigg\}.
\]
Note that the first condition is basically bounding the square of
\textbf{Mahalanobis distance} to a constant.

\paragraph{Advantages:}
This formulation often admits a \textbf{semi-definite programming} (SDP)
representation, making it computationally attractive.  

\paragraph{Limitations:}
From a statistical standpoint, $\mathcal{B}$ may contain distributions far from
the empirical distribution $\hat{\mathbb{P}}_n$, even for small $\delta_1,
\delta_2$. As $n \to \infty$, $\hat{\mathbb{P}}_n \overset{w.p. 1}{\rightarrow}
\mathbb{P}_\star$ under i.i.d.\ assumptions, but the set $\mathcal{B}$ may still
include distributions that may be far from $\mathbb{P_\star}$ (overly
conservative). This conservatism may hurt performance, except in cases where the
optimal solution only depends on first and second moments.  

\subsection{Metric-Based Uncertainty Sets}
A more modern and widely adopted approach defines the uncertainty set via a
\textbf{probability metric/discrepancy} between distributions.  
Specifically, given a discrepancy measure $D(\cdot, \cdot)$ on the space of
probability measures $\mathcal{P}(\Xi)$, define
\[
    \mathcal{B}_\delta(\hat{\mathbb{P}}_n) = \big\{ \mathbb{Q} \;:\; D(\mathbb{Q}, \hat{\mathbb{P}}_n) \leq \delta \big\}.
\]

Here, $\delta \geq 0$ defines the radius of a \textbf{ball} around
$\hat{\mathbb{P}}_n$ with respect to the chosen discrepancy measure $D$.  

\paragraph{Advantages:}
Such sets capture distributions \textit{close} to the empirical distribution
$\hat{\mathbb{P}}_n$.

\section{Distances between distributions}
\subsection{$\phi$-Divergence}
\subsubsection*{Absolute continuity}
A measure Q is said to be absolutely continuous w.r.t.\ measure P if for every
$A \in \mathcal{F}$,
\begin{align*}
    P(A) = 0 \, \Rightarrow \, Q(A) = 0\\
    \text{OR}\\
    Q(A) > 0 \, \Rightarrow \, P(A) > 0
\end{align*}
Thus, $Q \ll P$ (read as $P$ \emph{dominates} $Q$).

\subsubsection*{Continuous R.V.s}
Let $\mathbb{P}_X (A) = \mathbb{P}(X \in A)$ which forms a probability space
with $(\Omega, \mathcal{F}) \text{ where } \Omega = \mathbb{R}$, $X:\Omega \to
\mathbb{R}$ and $\mathcal{F}$ is Borel $\sigma$-field.  
If $\mathbb{P}_X(A)$ is absolutely continuous w.r.t.\ the Lebesgue measure
$\lambda$, then, by the \textbf{Radon-Nikodym theorem}, there exists a
$\mathcal{F}$-measurable function $p_X : X \to \left[0, \infty \right)$, such
that for any $A \in \mathcal{F}$,
\[
    \mathbb{P}_X(A) = \int_{X^{-1}(A)} \, dP = \int_{A} \, p_X \, d\lambda.
\]
The function $p_X$ satisfying the above equality is commonly written as
$\frac{d\mathbb{P}_X}{d\lambda}$ and is called \textbf{Radon-Nikodym
derivative}.\\
In probability theory, this term is known as probability density function of a
random variable. Usually, $p_X = p(X) \text{ and } A = \left[a, b\right]
\Rightarrow \mathbb{P}_X(\left[a, b\right]) = \int_{a}^{b} p(x) \, dx$.

\paragraph{Definition:} \ab{Of what? Use definition environment} Assume $\phi :
\mathbb{R}_{+} \to \left(-\infty, +\infty\right]$ is a convex function with
$\phi(0) = \lim_{t \to 0^+} \phi(t),$ then the $\phi$-divergence between
$\mathbb{Q}$ and $\hat{\mathbb{P}}_n$ is
\[
D_{\phi}(\mathbb{Q}, \hat{\mathbb{P}}_{n}) = 
    \begin{cases}
        \displaystyle \int_{\mathcal{Z}} \phi \left(\frac{d\mathbb{Q}}{d\hat{\mathbb{P}}_{n}} \right) d\hat{\mathbb{P}}_{n}(z), & \quad \mathbb{Q} \ll \hat{\mathbb{P}}_{n}, \\
        +\infty, & \quad \text{otherwise}.
    \end{cases}
\]
Where, $\tfrac{d\mathbb{Q}}{d\hat{\mathbb{P}}_{n}}$ = likelihood ratio
(Radonâ€“Nikodym derivative) and $\mathbb{Q}$ is absolutely continuous w.r.t.\
$\hat{\mathbb{P}}_n$.\\
The intuition using this is that the adversary can re-weight the relative
importance ($w_i = a_i$) of each sample with a budget constraint ($\delta$). So,
the adversary systematically explores how re-weighting can potentially impact
the performance of an estimator as measured by a given expected
loss~\cite{duchi2021learning}. The bigger the reweighting, the larger the
divergence.

\subsubsection*{Commonly used $\phi$-divergences:}
\begin{enumerate}
    \item KL divergence: $\phi(t) = t \log t$
    \item Reverse KL divergence: $\phi(t) = -\log t$
    \item Total Variation distance: $\varphi(t) = \tfrac{1}{2} |t - 1|$
    \item Jensen-Shannon divergence: $\varphi(t) = \tfrac{1}{2} \big( t \log t -
    (t+1)\log (\tfrac{t+1}{2}) \big)$
\end{enumerate}

\subsubsection*{KL Divergence:}
\textbf{Intuition:} 
\[
H(Q, P) - H(Q) 
= \sum_x Q(x) \log \frac{1}{P(x)} - \sum_x Q(x)\log \frac{1}{Q(x)}
\]
\[
\implies D_{\mathrm{KL}}(Q \| P) = \sum_x Q(x) \log \frac{Q(x)}{P(x)} 
\]

Thus, this implies Excess entropy from incorrect distribution.

\subsubsection*{Total Variation Distance}
\[
\mathrm{TV}(Q,P) = \int_{z} \frac{1}{2} \left| \frac{Q(x)}{P(x)} - 1 \right| P(x)\, dx 
= \frac{1}{2} \int_{z} |Q(x) - P(x)| dx 
= \sup_{A \in \mathcal{F}} |Q(A) - P(A)|
\]

\textbf{HW:} Show the last equality. Is total variation a metric?

\subsubsection*{Jensen-Shannon Divergence:} This is a symmetrized version of the KL-divergence.
\[
\mathrm{JS}(Q,P) = \tfrac{1}{2} D_{\mathrm{KL}}\!\left(Q \Big\| \tfrac{Q+P}{2}\right) 
+ \tfrac{1}{2} D_{\mathrm{KL}}\!\left(P \Big\| \tfrac{Q+P}{2}\right)
\]

\paragraph{Notes}
\begin{enumerate}
    \item $D_{\mathrm{KL}}$ is not a metric ($\because$ not symmetric, does not satisfies
    triangle inequality).
    \item Square root of $\mathrm{JS}$ is a metric.
    \item In $\phi$-divergence, $\mathbb{Q}$ is only supported where
    $\hat{\mathbb{P}}_n$ is supported.
\end{enumerate}

\subsubsection*{Properties of $\phi$-divergence}
\begin{enumerate}
\item \emph{Non-negativity}: For any two probability distributions $Q$ and $P$,
    \[
        D_\phi(Q, P) \geq 0.
    \]
    \begin{proof}
    From the definition,
    \[
        D_\phi(Q, P) = \int_{\mathcal{Z}} \phi\!\left(\frac{dQ}{dP}\right) dP,
    \]
    where $\phi : \mathbb{R}_+ \to (-\infty, +\infty]$ is convex with
    $\phi(1)=0$.\\
    By Jensen's inequality,
    \[
        \int_{\mathcal{Z}} \phi\!\left(\frac{dQ}{dP}\right) dP \;\; \geq \;\; 
        \phi\!\left( \int_{\mathcal{Z}} \frac{dQ}{dP} dP \right).
    \]
    Since $\int_{\mathcal{Z}} \frac{dQ}{dP} dP = \int_{\mathcal{Z}} dQ = 1$, the RHS equals
    $\phi(1)=0$.  
    Thus, $D_\phi(Q, P) \geq 0$.
    \end{proof}

\item \emph{Identity of indiscernibles}:
    \[
        D_\phi(Q, P) = 0 \quad \iff \quad Q \overset{a.s.}= P \quad (\text{up to measure 0 set}).
    \]
    \begin{proof}
    If $Q = P$, then $\tfrac{dQ}{dP}=1$ everywhere and hence
    \[
        D_\phi(Q,P) = \int_\Xi \phi(1) dP = 0.
    \]
    Conversely, if $D_\phi(Q,P)=0$, then by strict convexity of $\phi$ (minimum
    at $t=1$), we must have
    \[
        \frac{dQ}{dP} = 1 \quad \text{$P$-almost surely}.
    \]
    This implies $Q=P$.
    \end{proof}
\end{enumerate}

\bibliographystyle{plain}
\bibliography{refs}


% Optional, if you want to make a comment about vulnerability of any result or
%so on. \begin{danger} This is the danger environment. \end{danger}

\appendix
\chapter{Lebesgue integral with Dirac Measure}\label{adx1} Let $(X,\mathcal{F})$
be a measurable space and fix $x \in X$.  
The Dirac measure at $x$, denoted $\delta_x$, is defined by
\[
\delta_x(A) =
\begin{cases}
1, & x \in A, \\
0, & x \notin A,
\end{cases}
\quad \forall A \in \mathcal{F}.
\]
We want to show that for any measurable function $f : X \to [-\infty,\infty]$,
\[
\int_{x} f(y) \, d\delta_x(y) = f(x),
\]
with the usual convention that the integral may take values $\pm \infty$ if $f$
is not integrable.  
When $f$ is integrable (i.e. $\int |f| \, d\delta_x < \infty$), the value is a
finite real number equal to $f(x)$. \ab{Add citation to where the proof is from}
\section*{Proof}
\subsection*{Step 1: Indicator functions}

Let $A \in \mathcal{F}$. Then, by definition of the Lebesgue integral for
indicator functions,
\[
\int_{x} \mathbf{1}_A(y) \, d\delta_x(y) = \delta_x(A) =
\begin{cases}
1, & x \in A, \\
0, & x \notin A.
\end{cases}
\]
But note that $\mathbf{1}_A(x)$ equals the same right-hand side. Hence
\[
\int_{x} \mathbf{1}_A \, d\delta_x = \mathbf{1}_A(x).
\]

\subsection*{Step 2: Simple functions}

Let $s = \sum_{k=1}^n a_k \mathbf{1}_{A_k}$ be a (nonnegative) simple function
with $a_k \geq 0$ and $A_k \in \mathcal{F}$.  
Using linearity of the integral and Step 1,
\[
\int s \, d\delta_x = \sum_{k=1}^n a_k \int \mathbf{1}_{A_k} \, d\delta_x
= \sum_{k=1}^n a_k \, \mathbf{1}_{A_k}(x) = s(x).
\]

\subsection*{Step 3: Nonnegative measurable functions}

Let $f \geq 0$ be measurable. By construction of the Lebesgue integral, there
exists an increasing sequence of simple functions $(s_n)$ with $0 \leq s_n
\uparrow f$ pointwise.  
By the Monotone Convergence Theorem,
\[
\int f \, d\delta_x = \lim_{n \to \infty} \int s_n \, d\delta_x
= \lim_{n \to \infty} s_n(x) = f(x).
\]

\subsection*{Step 4: General measurable functions}

For a general measurable function $f$, write
\[
f = f^+ - f^-, \quad \text{where } f^+ = \max\{f,0\}, \; f^- = \max\{-f,0\}.
\]
If at least one of $\int f^\pm \, d\delta_x$ is $+\infty$, Step 3 implies
\[
\int f \, d\delta_x = f(x)
\]
in the extended sense.  
If $f$ is integrable (i.e. $\int |f| \, d\delta_x < \infty$), then applying Step
3 to $f^\pm$ and subtracting gives
\[
\int_{x} f \, d\delta_x = \int f^+ \, d\delta_x - \int f^- \, d\delta_x
= f^+(x) - f^-(x) = f(x).
\]



\end{document}